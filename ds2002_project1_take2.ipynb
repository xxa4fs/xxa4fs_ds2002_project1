{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b49811-b223-4f7e-bd4c-c6b122296765",
   "metadata": {},
   "source": [
    "### importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf1877-fc03-455b-8fd4-8fd6c4ae292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bd923-9493-456d-a161-fdfb889ae23b",
   "metadata": {},
   "source": [
    "### printing library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56de5a1-8beb-4ee1-a744-73c0c8906c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8cfc5-8c79-452a-bb24-f4f9d1707d30",
   "metadata": {},
   "source": [
    "### declaring and assigning connection variables to MySQL and MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbab8db-b90d-48a3-a9ac-4bf505ceca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_args = {\n",
    "    \"uid\": \"root\",\n",
    "    \"pwd\": \"new_password\",\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"dbname\": \"project1_dw\"  # target data warehouse database name\n",
    "}\n",
    "\n",
    "mongodb_args = {\n",
    "    \"user_name\": \"bob\",\n",
    "    \"password\": \"bob\",  \n",
    "    \"cluster_name\": \"cluster0\",  \n",
    "    \"cluster_subnet\": \"nqygc0x\", \n",
    "    \"cluster_location\": \"atlas\", \n",
    "    \"db_name\": \"project1_data\"  # MongoDB database name for source data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46ac4c-a98a-4ebb-b096-1e1199f9fc07",
   "metadata": {},
   "source": [
    "### defining functions for getting/setting data in MySQL and MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8d6f5-30c1-4164-9cdd-8c6a0ee8a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(text(sql_query), connection)\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql() function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        connection.execute(text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\"))\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name, data_directory, json_files):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770325a-04ab-432d-b1a5-c67d96a6af74",
   "metadata": {},
   "source": [
    "### create the data warehouse database if it doesn't exist yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fa421-1458-4e3e-8a13-73beaa01f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "connection = sqlEngine.connect()\n",
    "\n",
    "connection.execute(text(f\"DROP DATABASE IF EXISTS `{mysql_args['dbname']}`;\"))\n",
    "connection.execute(text(f\"CREATE DATABASE `{mysql_args['dbname']}`;\"))\n",
    "connection.execute(text(f\"USE {mysql_args['dbname']};\"))\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eebecc-d0e8-49fb-bbcd-95c7caee5018",
   "metadata": {},
   "source": [
    "### load customers dimension from csv file\n",
    "+ perform transformations (rename columns, modify number of columns) + add a surrogate key --> load to MySQL dim_customers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17141bc9-fe62-4599-a890-9c04ffd0ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = pd.read_csv('customers.csv')\n",
    "\n",
    "# perform some transformations\n",
    "df_customers.rename(columns={\n",
    "    'LastName': 'last_name',\n",
    "    'FirstName': 'first_name',\n",
    "}, inplace=True)\n",
    "\n",
    "if 'UnnecessaryColumn' in df_customers.columns:\n",
    "    df_customers.drop('UnnecessaryColumn', axis=1, inplace=True)\n",
    "\n",
    "# surrogate key\n",
    "df_customers.insert(0, 'customer_key', range(1, df_customers.shape[0] + 1))\n",
    "\n",
    "# insert dim_customers table\n",
    "table_name = 'dim_customers'\n",
    "primary_key = 'customer_key'\n",
    "db_operation = 'insert'\n",
    "\n",
    "set_dataframe(df_customers, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea141f-8861-4fd6-b801-3da4c22e950b",
   "metadata": {},
   "source": [
    "### upload products json to MongoDB and load products dimension \n",
    "+ gets the path of the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc11a1-a56b-4fd0-8ba6-1b997cb60c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd())\n",
    "\n",
    "json_files = {\"products\": 'products.json'}  # Adjust filename if needed\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44f193-9ef4-43ba-a94d-6bdaa8e63fbc",
   "metadata": {},
   "source": [
    "### extract products from MongoDB into dataframe and load to dim_products\n",
    "includes some transformations, adds a surrogate key --> load into MySQL dim_products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482d9c4-2a66-43e3-a248-f4c001702686",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "collection = \"products\"\n",
    "\n",
    "df_products = get_mongo_dataframe(client, mongodb_args[\"db_name\"], collection, query)\n",
    "\n",
    "df_products.rename(columns={\n",
    "    'ProductName': 'product_name',\n",
    "    'Category': 'category',\n",
    "}, inplace=True)\n",
    "\n",
    "if 'UnnecessaryField' in df_products.columns:\n",
    "    df_products.drop('UnnecessaryField', axis=1, inplace=True)\n",
    "\n",
    "# surrogate key\n",
    "df_products.insert(0, 'product_key', range(1, df_products.shape[0] + 1))\n",
    "\n",
    "# insert dim_products table\n",
    "table_name = 'dim_products'\n",
    "primary_key = 'product_key'\n",
    "db_operation = 'insert'\n",
    "\n",
    "set_dataframe(df_products, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eae639-57e9-451f-a731-bca2367184ff",
   "metadata": {},
   "source": [
    "### add date dimension \n",
    "Original file was written in T-SQL but the code below was rewritten for MySQL compatability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44accdc4-ecca-4175-9a73-9a45e815208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_sql = \"DROP TABLE IF EXISTS `dim_date`\"\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE `dim_date` (\n",
    "   `DateKey` INT NOT NULL, `Date` DATE NOT NULL, `Day` TINYINT NOT NULL, `DaySuffix` CHAR(2) NOT NULL,\n",
    "   `Weekday` TINYINT NOT NULL, `WeekDayName` VARCHAR(10) NOT NULL, `WeekDayName_Short` CHAR(3) NOT NULL,\n",
    "   `WeekDayName_FirstLetter` CHAR(1) NOT NULL, `DOWInMonth` TINYINT NOT NULL, `DayOfYear` SMALLINT NOT NULL,\n",
    "   `WeekOfMonth` TINYINT NOT NULL, `WeekOfYear` TINYINT NOT NULL, `Month` TINYINT NOT NULL,\n",
    "   `MonthName` VARCHAR(10) NOT NULL, `MonthName_Short` CHAR(3) NOT NULL, `MonthName_FirstLetter` CHAR(1) NOT NULL,\n",
    "   `Quarter` TINYINT NOT NULL, `QuarterName` VARCHAR(6) NOT NULL, `Year` INT NOT NULL, `MMYYYY` CHAR(6) NOT NULL,\n",
    "   `MonthYear` CHAR(8) NOT NULL, `IsWeekend` BIT NOT NULL, `IsHoliday` BIT NOT NULL,\n",
    "   `HolidayName` VARCHAR(50) NULL, `SpecialDays` VARCHAR(50) NULL, `FirstDateofYear` DATE NULL,\n",
    "   `LastDateofYear` DATE NULL, `FirstDateofMonth` DATE NULL, `LastDateofMonth` DATE NULL,\n",
    "   PRIMARY KEY (`DateKey`)\n",
    ")\n",
    "\"\"\"\n",
    "drop_procedure_sql = \"DROP PROCEDURE IF EXISTS PopulateDimDate\"\n",
    "create_procedure_sql = \"\"\"\n",
    "CREATE PROCEDURE PopulateDimDate(IN StartDate DATE, IN EndDate DATE)\n",
    "BEGIN\n",
    "    DECLARE CurrentDate DATE;\n",
    "    SET CurrentDate = StartDate;\n",
    "    WHILE CurrentDate <= EndDate DO\n",
    "        INSERT INTO `dim_date` (\n",
    "            `DateKey`, `Date`, `Day`, `DaySuffix`, `Weekday`, `WeekDayName`, `WeekDayName_Short`, `WeekDayName_FirstLetter`,\n",
    "            `DOWInMonth`, `DayOfYear`, `WeekOfMonth`, `WeekOfYear`, `Month`, `MonthName`, `MonthName_Short`, `MonthName_FirstLetter`,\n",
    "            `Quarter`, `QuarterName`, `Year`, `MMYYYY`, `MonthYear`, `IsWeekend`, `IsHoliday`, `FirstDateofYear`, `LastDateofYear`,\n",
    "            `FirstDateofMonth`, `LastDateofMonth`\n",
    "        )\n",
    "        SELECT\n",
    "            DATE_FORMAT(CurrentDate, '%Y%m%d'), CurrentDate, DAY(CurrentDate),\n",
    "            CASE WHEN DAY(CurrentDate) IN (1, 21, 31) THEN 'st' WHEN DAY(CurrentDate) IN (2, 22) THEN 'nd' WHEN DAY(CurrentDate) IN (3, 23) THEN 'rd' ELSE 'th' END,\n",
    "            DAYOFWEEK(CurrentDate), DAYNAME(CurrentDate), UPPER(LEFT(DAYNAME(CurrentDate), 3)), LEFT(DAYNAME(CurrentDate), 1),\n",
    "            DAYOFMONTH(CurrentDate), DAYOFYEAR(CurrentDate), FLOOR((DAYOFMONTH(CurrentDate) - 1) / 7) + 1, WEEKOFYEAR(CurrentDate),\n",
    "            MONTH(CurrentDate), MONTHNAME(CurrentDate), UPPER(LEFT(MONTHNAME(CurrentDate), 3)), LEFT(MONTHNAME(CurrentDate), 1),\n",
    "            QUARTER(CurrentDate),\n",
    "            CASE QUARTER(CurrentDate) WHEN 1 THEN 'First' WHEN 2 THEN 'Second' WHEN 3 THEN 'Third' WHEN 4 THEN 'Fourth' END,\n",
    "            YEAR(CurrentDate), DATE_FORMAT(CurrentDate, '%m%Y'), DATE_FORMAT(CurrentDate, '%Y-%b'),\n",
    "            CASE WHEN DAYNAME(CurrentDate) IN ('Saturday', 'Sunday') THEN 1 ELSE 0 END,\n",
    "            0, MAKEDATE(YEAR(CurrentDate), 1), STR_TO_DATE(CONCAT('12/31/', YEAR(CurrentDate)), '%m/%d/%Y'),\n",
    "            DATE_FORMAT(CurrentDate, '%Y-%m-01'), LAST_DAY(CurrentDate);\n",
    "        SET CurrentDate = DATE_ADD(CurrentDate, INTERVAL 1 DAY);\n",
    "    END WHILE;\n",
    "    UPDATE dim_date SET IsHoliday = 1, HolidayName = 'Christmas' WHERE Month = 12 AND Day = 25;\n",
    "    UPDATE dim_date SET SpecialDays = 'Valentines Day' WHERE Month = 2 AND Day = 14;\n",
    "END\n",
    "\"\"\"\n",
    "call_procedure_sql = \"CALL PopulateDimDate('2000-01-01', '2030-12-31');\"  # Expanded range to include 2001-07-01\n",
    "conn_str = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}/{mysql_args['dbname']}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "with sqlEngine.connect() as connection:\n",
    "    print(\"Executing: DROP TABLE...\")\n",
    "    connection.execute(text(drop_table_sql))\n",
    "       \n",
    "    print(\"Executing: CREATE TABLE...\")\n",
    "    connection.execute(text(create_table_sql))\n",
    "       \n",
    "    print(\"Executing: DROP PROCEDURE...\")\n",
    "    connection.execute(text(drop_procedure_sql))\n",
    "       \n",
    "    print(\"Executing: CREATE PROCEDURE...\")\n",
    "    connection.execute(text(create_procedure_sql))\n",
    "       \n",
    "    print(\"Executing: CALL PROCEDURE...\")\n",
    "    connection.execute(text(call_procedure_sql))\n",
    "       \n",
    "    connection.commit()\n",
    "print(\"\\nSuccessfully created and populated 'dim_date' table.\")\n",
    "\n",
    "# Verify\n",
    "df_date = get_sql_dataframe(\"SELECT * FROM dim_date LIMIT 2;\", **mysql_args)\n",
    "df_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa271-3bbb-43d4-8734-4e28877e8477",
   "metadata": {},
   "source": [
    "### use fact_sales_orders_vw to make a fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87437e76-32fc-4318-8662-414ae1c5970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying structure of fact_sales_orders_vw view in adventureworks...\")\n",
    "# temporary connect to adventureworks database\n",
    "conn_str_adventureworks = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}/adventureworks\"\n",
    "sqlEngine = create_engine(conn_str_adventureworks, pool_recycle=3600)\n",
    "with sqlEngine.connect() as connection:\n",
    "    sql_query = \"DESCRIBE fact_sales_orders_vw;\"\n",
    "    df_view_structure = pd.read_sql(sql_query, connection)\n",
    "\n",
    "    print(\"Columns in fact_sales_orders_vw view:\")\n",
    "    print(df_view_structure)\n",
    "\n",
    "    # sample view of the data\n",
    "    sql_sample_query = \"SELECT * FROM fact_sales_orders_vw LIMIT 5;\"\n",
    "    df_view_sample = pd.read_sql(sql_sample_query, connection)\n",
    "    print(\"\\nSample data from fact_sales_orders_vw view:\")\n",
    "    print(df_view_sample)\n",
    "\n",
    "print(\"Verification complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564039c7-a8d2-4b2e-888d-d7fd5c60f8e5",
   "metadata": {},
   "source": [
    "### extract and transform data for fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0dcaa-296a-46aa-be7c-3d7356672de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from adventureworks.fact_sales_orders_vw\n",
    "conn_str_adventureworks = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}/adventureworks\"\n",
    "sqlEngine_adventureworks = create_engine(conn_str_adventureworks, pool_recycle=3600)\n",
    "with sqlEngine_adventureworks.connect() as connection:\n",
    "    sql_query = \"SELECT * FROM fact_sales_orders_vw;\"\n",
    "    df_orders = pd.read_sql(sql_query, connection)\n",
    "    print(\"fact_sales_orders_vw columns and dtypes:\")\n",
    "    print(df_orders.dtypes)\n",
    "    print(\"\\nfact_sales_orders_vw sample (first 5 rows):\")\n",
    "    print(df_orders.head())\n",
    "\n",
    "# extract dimension keys from project1_dw\n",
    "conn_str_project1_dw = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}/{mysql_args['dbname']}\"\n",
    "sqlEngine_project1_dw = create_engine(conn_str_project1_dw, pool_recycle=3600)\n",
    "df_customers = get_sql_dataframe(\"SELECT CustomerID, customer_key FROM dim_customers;\", **mysql_args)\n",
    "df_products = get_sql_dataframe(\"SELECT ProductID, product_key FROM dim_products;\", **mysql_args)\n",
    "df_date = get_sql_dataframe(\"SELECT DateKey, Date FROM dim_date WHERE Date >= '2000-01-01' AND Date <= '2030-12-31';\", **mysql_args)\n",
    "\n",
    "# convert Date column in df_date to datetime64[ns]\n",
    "df_date['Date'] = pd.to_datetime(df_date['Date'])\n",
    "# convert OrderDate to datetime64[ns] \n",
    "df_orders['OrderDate'] = pd.to_datetime(df_orders['OrderDate']).dt.normalize()\n",
    "\n",
    "# dimension keys (adjust column names based on fact_sales_orders_vw output)\n",
    "df_fact = df_orders.merge(df_customers, left_on='CustomerID', right_on='CustomerID', how='left')\n",
    "print(\"\\nAfter customer merge:\")\n",
    "print(df_fact[['CustomerID', 'customer_key']].head())\n",
    "df_fact = df_fact.merge(df_products, left_on='ProductID', right_on='ProductID', how='left')\n",
    "print(\"\\nAfter product merge:\")\n",
    "print(df_fact[['ProductID', 'product_key']].head())\n",
    "df_fact = df_fact.merge(df_date, left_on='OrderDate', right_on='Date', how='left')\n",
    "print(\"\\nAfter date merge:\")\n",
    "print(df_fact[['OrderDate', 'Date', 'DateKey']].head())\n",
    "\n",
    "# Transform\n",
    "df_fact = df_fact[['SalesOrderID', 'customer_key', 'product_key', 'DateKey', 'OrderQty', 'UnitPrice']]\n",
    "df_fact.rename(columns={\n",
    "    'SalesOrderID': 'order_id',\n",
    "    'DateKey': 'date_key',\n",
    "    'OrderQty': 'quantity',\n",
    "    'UnitPrice': 'unit_price'\n",
    "}, inplace=True)\n",
    "df_fact.insert(0, 'fact_sales_key', range(1, df_fact.shape[0] + 1))\n",
    "\n",
    "# fill NaN values with 0 or drop\n",
    "df_fact['customer_key'] = df_fact['customer_key'].fillna(0).astype(int)\n",
    "df_fact['product_key'] = df_fact['product_key'].fillna(0).astype(int)\n",
    "df_fact['date_key'] = df_fact['date_key'].fillna(0).astype(int)\n",
    "\n",
    "# Calculate total amount (optional metric for the fact table)\n",
    "df_fact['total_amount'] = df_fact['quantity'] * df_fact['unit_price']\n",
    "\n",
    "print(\"\\nFinal fact table data before load:\")\n",
    "print(df_fact.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602278c-9ff9-4ede-af98-1a15b2abb20c",
   "metadata": {},
   "source": [
    "### loading fact table into data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9de4c2-d887-4010-b78b-ca880124d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'fact_sales'\n",
    "primary_key = 'fact_sales_key'\n",
    "db_operation = 'insert'\n",
    "\n",
    "set_dataframe(df_fact, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "print(f\"Fact table '{table_name}' loaded successfully.\")\n",
    "\n",
    "# Verify\n",
    "df_fact_result = get_sql_dataframe(f\"SELECT * FROM {table_name} LIMIT 10;\", **mysql_args)\n",
    "print(\"\\nVerification of fact_sales after reload:\")\n",
    "print(df_fact_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5913549-ca52-4763-a9f5-3f674f5a1c3d",
   "metadata": {},
   "source": [
    "### test SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890600b-fbce-4569-a38f-78a28a53a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query 1: Total sales quantity and amount per customer with AccountNumber\")\n",
    "sql_query1 = \"\"\"\n",
    "SELECT \n",
    "    c.customer_key, c.AccountNumber, \n",
    "    SUM(f.quantity) AS total_quantity, \n",
    "    SUM(f.total_amount) AS total_sales_amount\n",
    "FROM fact_sales f\n",
    "JOIN dim_customers c ON f.customer_key = c.customer_key\n",
    "JOIN dim_date d ON f.date_key = d.DateKey\n",
    "GROUP BY c.customer_key, c.AccountNumber\n",
    "ORDER BY total_sales_amount DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "df_query1 = get_sql_dataframe(sql_query1, **mysql_args)\n",
    "print(\"\\nQuery 1 Result: Total sales per customer with AccountNumber (top 5):\")\n",
    "print(df_query1)\n",
    "\n",
    "print(\"Query 2: Total quantity per product with ProductCategory\")\n",
    "sql_query2 = \"\"\"\n",
    "SELECT \n",
    "    p.product_key, p.ProductCategory, \n",
    "    SUM(f.quantity) AS total_quantity\n",
    "FROM fact_sales f\n",
    "JOIN dim_products p ON f.product_key = p.product_key\n",
    "JOIN dim_date d ON f.date_key = d.DateKey\n",
    "GROUP BY p.product_key, p.ProductCategory\n",
    "ORDER BY total_quantity DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "df_query2 = get_sql_dataframe(sql_query2, **mysql_args)\n",
    "print(\"\\nQuery 2 Result: Total quantity per product with category (top 5):\")\n",
    "print(df_query2)\n",
    "\n",
    "print(\"SQL queries executed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
